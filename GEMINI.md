# 제미니 프로젝트 가이드 문서

민짱님, 안녕하세요! 이 문서는 aiga-llm-server 프로젝트의 이해를 돕기 위해 작성된 제미니의 내부 가이드입니다. 민짱님의 요청을 정확하고 효율적으로 수행하기 위해 항상 이 문서를 먼저 참고하겠습니다.

## 1. 프로젝트 개요

이 프로젝트는 사용자에게 건강 관련 정보를 제공하는 LLM 기반 챗봇 서비스입니다. 사용자가 채팅을 통해 질문하면, AI 에이전트가 질문 의도를 파악하고 데이터베이스(DB)에서 병원, 의사 정보를 조회하거나 일반적인 건강 정보를 제공합니다.

## 2. 핵심 동작 흐름

1.  **API 수신 (`chat.py`)**: 사용자의 메시지를 FastAPI 엔드포인트에서 받습니다. 민감 단어 필터링 등 초기 전처리를 수행합니다.
2.  **서비스 로직 실행 (`service.py`)**: `startQuery` 함수를 통해 LangGraph로 구현된 AI 에이전트 실행을 시작합니다.
3.  **에이전트 워크플로우 (`agent.py`)**:
    *   `agent_node`: 사용자 질문, 위치 정보(GPS), 대화 기록 등을 바탕으로 LLM을 호출하여 다음 행동(도구 사용 또는 답변)을 결정합니다.
    *   `custom_tool_node`: 결정된 도구를 실행합니다. 도구 실행 결과가 비어있으면, 다른 도구로 재시도하는 폴백(Fallback) 로직이 포함되어 있습니다.
    *   `validate_node`: 생성된 답변이 적절한지 검증합니다.
4.  **도구 실행 (`tools.py` & `sql_tool.py`)**:
    *   질문 유형에 따라 적절한 도구를 사용해 DB에서 정보를 조회합니다.
    *   단순 조회는 일반 도구(`recommend_doctor` 등)를 사용합니다.
    *   복잡한 자연어 질문은 SQL Agent(`search_doctor_for_else_question`)를 사용하여 질문을 SQL 쿼리로 변환 후 실행합니다.
5.  **응답 생성 및 반환**:
    *   도구 실행 결과를 바탕으로 `agent_node`가 최종 답변을 생성합니다.
    *   `service.py`의 `makeResponse` 함수가 이 답변을 클라이언트 요구사항에 맞는 JSON 형식으로 가공합니다.
    *   `chat.py`를 통해 사용자에게 최종 응답을 전송합니다.

## 3. 주요 파일 설명

-   `@app/routers/chat.py`
    *   **역할**: API의 진입점(Entrypoint).
    *   **주요 기능**: `/start`, `/stop` 등 채팅 관련 HTTP 요청을 받아 `service.py`의 함수를 호출합니다. `sanitize_prompt`를 통해 사용자 메시지 내 민감 단어를 사전에 필터링합니다.

-   `@app/services/service.py`
    *   **역할**: 핵심 비즈니스 로직 처리.
    *   **주요 기능**: `LangGraphExecutionManager`를 통해 AI 에이전트의 비동기 실행을 관리합니다. 에이전트의 실행 결과를 `makeResponse` 함수를 통해 최종 API 응답 포맷으로 가공합니다.

-   `@app/agent.py`
    *   **역할**: **프로젝트의 심장.** LangGraph를 사용하여 AI 에이전트의 전체 워크플로우를 정의합니다.
    *   **주요 기능**:
        *   `AgentState`: 대화 메시지, 사용자 위치(GPS, locale), 재시도 횟수 등 대화의 상태를 관리.
        *   `agent_node`: 시스템 프롬프트에 위치 정보, 언어 규칙 등을 동적으로 주입하고 LLM을 호출.
        *   `custom_tool_node`: 도구를 실행하고, 결과가 비었을 때 다른 도구로 대체 실행하는 폴백 로직 담당.
        *   **대화 요약/복원**: 긴 도구 결과를 DB에 저장(`tool_results_cache` 테이블)하고 요약된 정보만 대화 기록에 남겨 토큰을 절약합니다.

-   `@app/tools/tools.py`
    *   **역할**: AI 에이전트가 사용하는 핵심 도구(Tool) 정의.
    *   **주요 기능**:
        *   `recommend_doctor`, `search_doctor` 등: 특정 조건에 따라 DB에서 직접 정보를 조회하는 간단한 도구.
        *   `search_doctor_for_else_question`: **가장 강력하고 복잡한 도구.** LangChain SQL Agent를 사용하여 복잡한 자연어 질문을 SQL로 변환하고 실행합니다.
        *   `get_cached_tool_result`: `agent.py`에서 DB에 저장한 과거 도구 실행 결과를 ID로 다시 불러오는 내부 관리용 도구.

-   `@app/tools/sql_tool.py`
    *   **역할**: SQL Agent를 보조하는 하위 도구 정의.
    *   **주요 기능**: `search_hospitals_by_location_and_department`처럼, '지역+진료과' 등 정형화된 조건으로 DB를 검색하는 함수들이 포함되어 있습니다. 이는 복잡한 SQL Agent 대신 더 빠르고 정확한 조회를 가능하게 합니다.

-   `@app/prompt/system_prompt.py`
    *   **역할**: AI 에이전트의 행동 지침.
    *   **주요 기능**: 에이전트의 역할, 도구 선택 규칙, 답변 생성 스타일, 제약 조건 등이 매우 상세하게 정의되어 있습니다. **에이전트의 모든 행동은 이 프롬프트에 기반합니다.**

## 4. 특이사항 및 숙지할 점

*   **대화 컨텍스트 관리**: 대화 기록이 길어지면 `agent.py`의 로직에 따라 과거 도구 실행 결과(`ToolMessage`)가 DB에 저장되고, 대화창에는 `{ "migrated": true, "result_id": "..." }` 형태의 요약 정보만 남습니다. 과거 정보를 다시 참조해야 할 때는 `get_cached_tool_result` 도구를 사용해야 합니다.
*   **위치 기반 검색**: 사용자 질문에 '근처', '가까운' 등의 키워드가 있거나, `latitude`, `longitude` 값이 주어지면 프롬프트에 관련 정보가 주입되고, 이를 바탕으로 도구들이 거리 기반 검색을 수행합니다.
*   **SQL Agent 동작**: `search_doctor_for_else_question` 도구는 `use_json_output` 파라미터 값에 따라 다른 시스템 프롬프트를 사용하여, 결과물을 구조화된 JSON으로 받을지, 아니면 일반 텍스트 요약으로 받을지 결정합니다.

이 문서를 기반으로 민짱님의 요청을 더욱 정확하게 파악하고 처리하도록 하겠습니다.

---
## 2026년 2월 19일: LLM 툴 선택 및 컨텍스트 관리 로직 개선

**1. `SYSTEM_PROMPT` 로직 대폭 수정 (`app/prompt/system_prompt.py`)**
   - **3단계 의사결정 프로세스 도입**: `[1단계: 의도 파악] -> [2단계: 엄격한 툴 매핑] -> [3단계: 컨텍스트 상속]` 구조로 변경하여, 단순 정보 질문과 툴 사용 요청을 명확히 분리하고, 툴 선택의 정확성을 높였습니다.
   - **전체 툴 매핑 규칙 정의**: `sql_tool.py`에 있는 모든 툴을 포함하여, 6가지 핵심 인자 조합에 따른 16가지의 상세한 툴 선택 규칙을 완성했습니다.
   - **출력 규칙 강화**: 툴 실행 결과가 있을 때와 없을 때의 답변 생성 방식을 명확히 구분하고, 특히 과거의 툴 결과를 반복하거나, 결과가 있는데도 없다고 답변하는 오류를 방지하는 `[절대 규칙]`을 추가했습니다.

**2. `agent.py`의 툴 리스트 및 상태 관리 수정**
   - **`external_tools` 등록**: `SYSTEM_PROMPT`에 추가된 모든 툴이 실제로 호출될 수 있도록 `external_tools` 리스트에 누락된 툴들을 모두 추가했습니다.
   - **`AgentState` 타입 수정**: `entity_history`의 타입 힌트를 `list`에서 `dict`로 변경하여, 엔티티 정보가 유실될 수 있는 잠재적 오류를 수정했습니다.

**3. `entity_analyzer.py`의 엔티티 관리 로직 개선**
   - **엔티티 파괴 로직 제거**: 특정 엔티티(예: 진료과)가 존재할 때 다른 중요 엔티티(예: 질환명)를 삭제하던 로직을 제거하여, 모든 컨텍스트가 보존되도록 했습니다.
   - **적극적 컨텍스트 병합**: 현재 질문의 정보와 `entity_history`의 과거 정보를 항상 병합하도록 변경하여, 대화가 이어질 때 컨텍스트가 유실되지 않도록 수정했습니다.
   - **그룹 지역명 인식 강화**: `extract_entities_for_routing`의 LLM 프롬프트에 '부울경'과 같은 그룹 지역명을 `location`으로 인식하도록 규칙을 추가했습니다.

**4. `sql_tool.py`의 좌표 검색 로직 수정**
   - **`handle_proximity_search` 수정**: "춘천 근처"와 같이 명시적인 지역명과 함께 '근처'를 검색할 때, 사용자의 현재 GPS가 아닌 명시된 지역('춘천')의 좌표를 우선적으로 조회하도록 로직을 수정했습니다.

---
## 2026년 2월 26일: LLM 툴 선택 및 컨텍스트 관리 로직 개선

**1. `agent_node` 내 내부 캐시 복원 루프 도입 (`app/agent.py`)**
   - **순차적 툴 호출 해결**: LLM이 `get_cached_tool_result`를 호출할 경우, 그래프를 종료하지 않고 즉시 SQLite에서 데이터를 조회하여 메시지에 삽입한 후 모델을 재호출하는 '내부 루프'를 구현했습니다. 이를 통해 [캐시 복원 -> 상세 검색] 과정이 단일 턴에서 끊김 없이 처리됩니다.

**2. 선제적 핵심 정보 복원(Proactive Refined Restoration) 로직 추가**
   - **토큰 최적화 및 정확도 향상**: 대화가 길어질 경우(2턴 이상), 과거 캐시 데이터 전체를 복원하는 대신 **의사명, 병원명, 진료과, 전문분야** 등 핵심 엔티티만 추출하여 미리 컨텍스트에 주입합니다. 이를 통해 LLM이 과거 인물을 짐작해서 답변하는 오류를 방지했습니다.
   - **설정값 연동**: `app/config.py`의 `proactive_restoration_limit` 설정을 통해 복원할 과거 캐시 개수를 유연하게 조절할 수 있게 했습니다.

**3. 데이터 무결성 및 '이중 마이그레이션' 버그 수정**
   - **DB 오염 차단**: 복원된 요약 데이터가 다시 마이그레이션 로직에 걸려 원본 SQLite 데이터를 덮어쓰는 문제를 해결했습니다. `is_historical_context` 및 `migrated` 플래그를 철저히 관리하여 원본 데이터는 보존하고 메모리상에서만 정보를 활용하도록 수정했습니다.

**4. `SYSTEM_PROMPT` 툴 매핑 및 규칙 강화 (`app/prompt/system_prompt.py`)**
   - **`search_doctor` 정밀도 향상**: 동명이인 문제를 해결하기 위해 [이름+병원명], [이름+진료과] 조합 시 `search_doctor`를 최우선으로 사용하도록 규칙을 재정의했습니다. 또한 특정 인물 지목 시 히스토리의 소속 정보를 반드시 포함하도록 강제했습니다.
   - **화제 전환(Topic Switch) 지침 추가**: 과거 컨텍스트에 얽매이지 않도록, 사용자의 현재 질문에 새로운 조건(새로운 질환, 지역 등)이 포함되면 과거 데이터보다 현재 질문의 인자를 최우선하도록 지침을 강화했습니다.

**5. 로깅 및 가시성 개선**
   - **복원 프로세스 추적**: 로그에 `[# 1]`, `[# 2]`와 같이 복원 순번을 표시하고, 복원된 엔티티 정보를 계층형으로 출력하여 어떤 과거 맥락이 현재 답변에 기여하는지 한눈에 확인할 수 있게 개선했습니다.