import os
import asyncio
import json
import uuid # ğŸš¨ Add uuid for generating unique IDs
import re
from .tools.location_dic import LOCATION_NORMALIZATION_RULES, GROUP_LOCATION_EXPANSION_RULES
from .common.location_analyzer import classify_location_query, analyze_other_location_request, update_location_context
from .common.entity_analyzer import update_entity_context, extract_entities_for_routing, extract_entities_for_routing_only_find_dept, extract_entities_from_ai_response_and_update_history
from .common.utils import is_result_empty

from .common.geocoder import get_address_from_coordinates
from .introduce import EMERGENCY_INTRODUCTION 

from .common.handlers import classify_and_handle_initial_requests

from langchain_openai import AzureChatOpenAI
from langgraph.graph import StateGraph, END, START
from typing import TypedDict, Annotated, Literal, Optional, Optional
from .config import settings
from langchain_core.tools import tool
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import MessagesState, add_messages
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from openai import APITimeoutError
from langchain_core.runnables.config import RunnableConfig

from .tools.tools import recommand_doctor, recommend_hospital, search_doctor, search_doctor_by_hospital, search_doctor_for_else_question
from .tools.tools import get_cached_tool_result # ë‚´ë¶€ ìƒíƒœ ì¡°íšŒë¥¼ ìœ„í•œ ë©”íƒ€ ë„êµ¬
from .tools.sql_tool import (
    search_hospitals_by_location_and_department,
    search_hospital_by_disease_and_location,
    search_hospital_by_disease,
    search_hospital_by_disease_and_department,
    search_doctors_by_location_and_department,
    search_doctors_by_disease_and_location,
    search_by_location_only,
    search_doctors_by_department_only,
    search_doctors_by_disease_and_department,
    search_doctor_details_by_name,
    search_hospital_details_by_name,
    search_doctors_by_hospital_name
)
from .prompt.system_prompt import SYSTEM_PROMPT
from .prompt.validation_prompt import VALIDATION_PROMPT
from .common.logger import logger
from .tools.language_set import LANGUAGE_SET, LANGUAGE_GREETINGS, DEFAULT_GREETING
from .common.sanitizer import sanitize_prompt

import aiosqlite
import json
import uuid # ğŸš¨ Add uuid for generating unique IDs

# ëª¨ë¸ ì„¤ì •
llm = AzureChatOpenAI(
    model_name=settings.azure_api_model,
    azure_endpoint=settings.azure_endpoint,
    api_key=settings.azure_key,
    api_version=settings.azure_api_version,
    temperature=0,
    request_timeout=settings.azure_request_timeout
)

llm_for_summary = AzureChatOpenAI(
    model_name=settings.azure_summary_api_model, # TODO: Use a cheaper model for summarization
    azure_endpoint=settings.azure_endpoint,
    api_key=settings.azure_key,
    api_version=settings.azure_api_version,
    temperature=0,
    request_timeout=settings.azure_request_timeout
)

# 5ê°œì˜ ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ ë„êµ¬
external_tools = [
    recommand_doctor, 
    recommend_hospital, 
    search_doctor, 
    search_doctor_by_hospital, 
    search_doctor_for_else_question,
    search_hospital_by_disease,
    search_doctors_by_department_only,
    search_doctors_by_disease_and_department,
    search_doctors_by_location_and_department,
    search_hospitals_by_location_and_department,
    search_doctors_by_disease_and_location,
    search_hospital_by_disease_and_location,
    search_hospital_by_disease_and_department,
    search_by_location_only,
    search_doctor_details_by_name,
    search_hospital_details_by_name,
    search_doctors_by_hospital_name
]
# 1ê°œì˜ ë‚´ë¶€ ìƒíƒœ ê´€ë¦¬ ë„êµ¬
internal_tools = [get_cached_tool_result]

# LLM ë°”ì¸ë”©ì„ ìœ„í•´ ë‘ ë¦¬ìŠ¤íŠ¸ ê²°í•©
tools = external_tools + internal_tools
tool_map = {t.name: t for t in tools}
model = llm.bind_tools(tools)


class AgentState(MessagesState):
    messages: Annotated[list, add_messages]
    locale: Annotated[str, ""]
    latitude: Optional[float]
    longitude: Optional[float]
    # The location_history stores a timeline of location contexts.
    # Each entry is a dict, e.g., {"type": "GPS", ...} or {"type": "CONTEXTUAL", ...}
    location_history: Annotated[list, []]
    # The entity_history stores a timeline of core medical entity contexts.
    entity_history: Annotated[dict, {}]
    # location: Annotated[Optional[str], None] # ì¶”ê°€: ëª…ì‹œì  location í•„ë“œ - entity_historyë¡œ ì´ë™
    retry: Annotated[int, 0]
    valid: Annotated[bool, False]
    summary_input_tokens: Annotated[int, 0]
    summary_output_tokens: Annotated[int, 0]
    summary_total_tokens: Annotated[int, 0]
    last_ai_message: Optional[str] # ì´ì „ AI ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ í•„ë“œ ì¶”ê°€

# 1ï¸âƒ£ ì—ì´ì „íŠ¸ ë…¸ë“œ
async def agent_node(state: AgentState, config: RunnableConfig):
    """ëª¨ë¸ì„ í†µí•´ ë‹µë³€í•˜ëŠ” Agent ë…¸ë“œ. ì½˜í…ì¸  í•„í„° ì—ëŸ¬ ë°œìƒ ì‹œ, ë©”ì‹œì§€ë¥¼ ì •ì œí•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤."""
    
    # ğŸš¨ ì¶”ê°€: last_ai_messageê°€ stateì— ì—†ì„ ê²½ìš° ì´ˆê¸°í™”
    if "last_ai_message" not in state:
        state["last_ai_message"] = None
    
    current_user_message = ""
    if state["messages"] and isinstance(state["messages"][-1], HumanMessage):
        current_user_message = state["messages"][-1].content

    num_human_messages = len([msg for msg in state['messages'] if isinstance(msg, HumanMessage)])
    is_first_interaction_in_session = (num_human_messages == 1)
    locale = state.get("locale") or "ko"

    # --- START: ì´ˆê¸° ìš”ì²­ í†µí•© ì²˜ë¦¬ (í˜„ì¬ ìœ„ì¹˜, ì‘ê¸‰ ìƒí™©, ê¸ˆì§€ëœ ì¶”ì²œ) ---
    response = await classify_and_handle_initial_requests(state, config, current_user_message, is_first_interaction_in_session, locale)
    if response:
        return response
    # --- END: ì´ˆê¸° ìš”ì²­ í†µí•© ì²˜ë¦¬ ---

    # --- Start of New Location Context Management ---
    
    # 1. Call the new location context manager
    updated_history, clarification_question = await update_location_context(
        llm=llm_for_summary, # Pass the llm instance
        user_message=current_user_message,
        location_history=state.get('location_history', []),
        latitude=state.get('latitude'),
        longitude=state.get('longitude')
    )

    # 2. Update state with the new history
    state['location_history'] = updated_history

    # ì—”íŠ¸ë¦¬ ì €ì¥ ì—¬ë¶€  ê°ì§€ (í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì‹œ)
    # ğŸš¨ START: AIì˜ ìµœì¢… ë‹µë³€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œí•˜ì—¬ entity_history ì—…ë°ì´íŠ¸
    current_entity_history = state.get("entity_history")
    if current_entity_history is None:
        current_entity_history = {"hospitals": [], "doctors": [], "departments": [], "diseases": [], "location": None}
    
    # state["last_ai_message"]ê°€ ì¡´ì¬í•  ë•Œë§Œ ì—”í‹°í‹° ì¶”ì¶œ ë¡œì§ ì‹¤í–‰
    # --- START: ì§ì „ AIMessage ë¶„ì„ì„ í†µí•œ ë ˆê±°ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    ai_message_recommendation_info = "" # ìµœì¢… system promptì— ì‚½ì…ë  ì •ë³´
    last_ai_message_content = None # ì§ì „ AIMessageì˜ contentë¥¼ ì €ì¥í•  ë³€ìˆ˜
    
    # ë§ˆì§€ë§‰ HumanMessageì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    last_human_message_idx = -1
    for i in range(len( state["messages"]) - 1, -1, -1):
        if isinstance( state["messages"][i], HumanMessage):
            last_human_message_idx = i
            break

    # ë§ˆì§€ë§‰ HumanMessage ì´ì „ì— AIMessageê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    # ì¦‰, last_human_message_idx - 1 ìœ„ì¹˜ì˜ ë©”ì‹œì§€ê°€ AIMessageì—¬ì•¼ í•©ë‹ˆë‹¤.
    if last_human_message_idx > 0 and isinstance( state["messages"][last_human_message_idx - 1], AIMessage):
        last_ai_message_content =  state["messages"][last_human_message_idx - 1].content
    # --- END: ì§ì „ AIMessage ë¶„ì„ì„ í†µí•œ ë ˆê±°ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ---

    last_ai_message_to_process = last_ai_message_content
    if last_ai_message_to_process:
        state["entity_history"] = await extract_entities_from_ai_response_and_update_history(
            llm=llm_for_summary,
            ai_response_content=last_ai_message_to_process,
            current_entity_history=current_entity_history
        )
    else:
        state["entity_history"] = current_entity_history # last_ai_messageê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ entity_history ìœ ì§€ ë˜ëŠ” ì´ˆê¸°í™”
    # ğŸš¨ END: AIì˜ ìµœì¢… ë‹µë³€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œí•˜ì—¬ entity_history ì—…ë°ì´íŠ¸
    # --- End of New Entity Context Management ---

    # 3. If the manager returned a question, ask it immediately.
    if clarification_question:
        greeting = LANGUAGE_GREETINGS.get(locale, DEFAULT_GREETING)
        if is_first_interaction_in_session:
            clarification_question = f"{greeting}\n\n  {clarification_question}"
        logger.info(f"Asking clarification question from location manager: {clarification_question}")
        response = AIMessage(content=clarification_question)
        return {
            "messages": [response], 
            "retry": state.get("retry", 0), 
            "valid": True, 
            "location_history": state['location_history'],
            "entity_history": state['entity_history']
        }

    # 5. Prepare entity information to be injected into the main prompt as a structured JSON block
    persistent_facts_info = ""
    # ğŸš¨ ìˆ˜ì •: state['entity_history']ê°€ ì¡´ì¬í•˜ê³  ë‚´ìš©ì´ ìˆì„ ê²½ìš° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
    if state.get("entity_history") and any(state["entity_history"].values()):
        persistent_facts_info += f"""
/*
IMPORTANT: The following JSON block contains the latest confirmed entities from the conversation history.
The AI's previous message contained the following recommendations or suggestions. The user's current message might be an acceptance or follow-up on these. Prioritize the following information if the user's intent aligns with these details.
Inherited entities:
*/
{json.dumps(state["entity_history"], ensure_ascii=False, indent=2)}
"""
    
    messages = state["messages"] 

    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ì •ë³´ ì¤€ë¹„
    current_latitude = state.get('latitude')
    current_longitude = state.get('longitude')
    location_gps_info = ""
    if current_latitude is not None and current_longitude is not None:
        location_gps_info = f"\n\n/*\nIMPORTANT: User's current GPS location is available.\nLatitude: {current_latitude}\nLongitude: {current_longitude}\nPrioritize using location-based tools if the user asks for nearby facilities.\n*/\n"
    
    locale = state.get("locale") or "ko"
    language_name = LANGUAGE_SET.get(locale, LANGUAGE_SET["ko"])
    language_rule = f"\n\n**Response Language Rule**\n- The AI counselor's final response MUST be generated in **{language_name}**.\n"

    # 2. ê¸°ë³¸ SYSTEM_PROMPTì—ì„œ ê¸°ì¡´ GPS ì •ë³´ ë¸”ë¡ ë° ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
    # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ SYSTEM_PROMPT ìì²´ë¥¼ í´ë¦¬ë‹
    clean_system_prompt_base = re.sub(r'\[ì‚¬ìš©ì í˜„ì¬ ìœ„ì¹˜ ì •ë³´ \(GPS\)\].*?\[ì§€ì—­ ì‚¬ì „ ë¶„ì„ í”Œë˜ê·¸\]', '', SYSTEM_PROMPT, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'\n+/\*.*?IMPORTANT:.*?\*/\n*', '', clean_system_prompt_base, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'\[ì§€ì—­ ì‚¬ì „ ë¶„ì„ í”Œë˜ê·¸\].*?---', '', clean_system_prompt_base, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'\[Location Context\].*?\}', '', clean_system_prompt_base, flags=re.DOTALL)

    # 3. ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    final_system_prompt_content = clean_system_prompt_base + persistent_facts_info + location_gps_info + language_rule
    
    # 4. messages ë¦¬ìŠ¤íŠ¸ì—ì„œ ê¸°ì¡´ SystemMessageë¥¼ ëª¨ë‘ ì œê±°
    messages[:] = [msg for msg in messages if not isinstance(msg, SystemMessage)]
    
    # 5. ìµœì¢… êµ¬ì„±ëœ SystemMessageë¥¼ ë§¨ ì•ì— ì‚½ì…
    messages.insert(0, SystemMessage(content=final_system_prompt_content))

    # --- START: ToolMessage ë§ˆì´ê·¸ë ˆì´ì…˜ (ì••ì¶• ë° ìºì‹±) ---
    # ëŒ€ìš©ëŸ‰ ToolMessageë¥¼ SQLiteì— ì €ì¥í•˜ê³  ìš”ì•½ ì •ë³´ë¡œ ëŒ€ì²´í•˜ì—¬ í† í°ì„ ì ˆì•½í•©ë‹ˆë‹¤.
    if settings.llm_summary_verbose:
        session_id = config["configurable"]["thread_id"]
        async with aiosqlite.connect(settings.sqlite_directory, check_same_thread=False) as conn:
            # í˜„ì¬ í„´ì—ì„œ ë°©ê¸ˆ ì‹¤í–‰ëœ ìµœì‹  ToolMessageëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ ì œì™¸í•œë‹¤.
            latest_tool_messages_indices = set()
            if len(messages) > 1 and isinstance(messages[-1], ToolMessage):
                if isinstance(messages[-2], AIMessage) and messages[-2].tool_calls:
                    num_tool_calls = len(messages[-2].tool_calls)
                    for i in range(num_tool_calls):
                        idx_to_exclude = len(messages) - 1 - i
                        if idx_to_exclude >= 0 and isinstance(messages[idx_to_exclude], ToolMessage):
                             latest_tool_messages_indices.add(idx_to_exclude)
                        else:
                            break

            # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° 'ê³¼ê±°ì˜' ToolMessageë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜
            for i in range(len(messages) - 1, -1, -1):
                if i in latest_tool_messages_indices:
                    continue
                msg = messages[i]
                if isinstance(msg, ToolMessage):
                    try:
                        tool_content_json = json.loads(msg.content)
                        # ğŸš¨ [BUG FIX] ì´ë¯¸ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆê±°ë‚˜, ë³µì›ëœ ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ì§€ ì•ŠìŒ (DB ë®ì–´ì“°ê¸° ë°©ì§€)
                        if not isinstance(tool_content_json, dict) or tool_content_json.get("migrated") is True or tool_content_json.get("is_historical_context") is True:
                            continue
                        
                        original_content = msg.content
                        result_id = str(uuid.uuid4())
                        await conn.execute(
                            "INSERT OR REPLACE INTO tool_results_cache (session_id, result_id, content) VALUES (?, ?, ?)",
                            (session_id, result_id, original_content)
                        )
                        await conn.commit()
                        
                        placeholder_summary = "ê³¼ê±° ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ì™¸ë¶€ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
                        param_dict = {}
                        if 'chat_type' in tool_content_json:
                            answer_content = tool_content_json.get('answer')
                            if isinstance(answer_content, dict):
                                summary_parts = []
                                count_info = ""
                                if answer_content.get('disease'):
                                    summary_parts.append(f"ì§ˆí™˜: {answer_content['disease']}")
                                    param_dict['disease'] = answer_content['disease']
                                if answer_content.get('department'):
                                    summary_parts.append(f"ì§„ë£Œê³¼: {answer_content['department']}")
                                    param_dict['department'] = answer_content['department']
                                if answer_content.get('hospital'):
                                    summary_parts.append(f"ë³‘ì›: {answer_content['hospital']}")
                                    param_dict['hospital'] = answer_content['hospital']
                                elif answer_content.get('hospitals') and len(answer_content['hospitals']) > 0:
                                    first_hosp = answer_content['hospitals'][0].get('name', '')
                                    if first_hosp: summary_parts.append(f"ì£¼ìš” ë³‘ì›: {first_hosp}")
                                    param_dict['hospital'] = first_hosp
                                    param_dict['hospital_count'] = len(answer_content['hospitals'])
                                if answer_content.get('doctors') and len(answer_content['doctors']) > 0:
                                    first_doc = answer_content['doctors'][0].get('name', '')
                                    if first_doc: summary_parts.append(f"ì£¼ìš” ì˜ì‚¬: {first_doc}")
                                    param_dict['doctor'] = first_doc
                                    param_dict['doctor_count'] = len(answer_content['doctors'])
                                
                                if answer_content.get('doctors'): count_info = f"{len(answer_content['doctors'])}ëª…ì˜ ì˜ì‚¬ ì •ë³´"
                                elif answer_content.get('hospitals'): count_info = f"{len(answer_content['hospitals'])}ê°œì˜ ë³‘ì› ì •ë³´"

                                if summary_parts or count_info:
                                    placeholder_summary = f"ê³¼ê±° {tool_content_json['chat_type']} ê²°ê³¼: {count_info}{' (' + ', '.join(summary_parts) + ')' if summary_parts else ''}"
                            elif isinstance(answer_content, str):
                                placeholder_summary = f"ê³¼ê±° {tool_content_json['chat_type']} ê²°ê³¼: {answer_content[:100]}... (ì €ì¥ë¨)"

                        msg.content = json.dumps({
                            "migrated": True,
                            "result_id": result_id,
                            "summary": placeholder_summary,
                            "param": param_dict
                        }, ensure_ascii=False)
                        logger.info(f"ToolMessage migrated. result_id: {result_id}, summary: {placeholder_summary}")
                    except Exception as e:
                        logger.error(f"Error during ToolMessage migration: {e}")
    # --- END: ToolMessage ë§ˆì´ê·¸ë ˆì´ì…˜ ---

    # --- START: Proactive Refined Restoration (ì„ ì œì  í•µì‹¬ ì •ë³´ ë³µì›) ---
    # ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆ ê²½ìš°(2í„´ ì´ìƒ), ê³¼ê±° ìºì‹œì—ì„œ í•µì‹¬ ì—”í‹°í‹°ë§Œ ë½‘ì•„ ë¯¸ë¦¬ ë³µì›í•˜ì—¬ í† í°ì„ ì•„ë¼ê³  ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    session_id = config["configurable"]["thread_id"]
    migrated_tool_messages = [msg for msg in messages if isinstance(msg, ToolMessage) and '"migrated": true' in msg.content]
    
    if len(migrated_tool_messages) > 0:
        limit = settings.proactive_restoration_limit
        logger.info(f"--- [PROACTIVE RESTORATION] {len(migrated_tool_messages)}ê°œì˜ ìºì‹œ ì¤‘ ìµœê·¼ {limit}ê°œ í•µì‹¬ ì •ë³´ ë³µì› ì‹œë„ ---")
        async with aiosqlite.connect(settings.sqlite_directory, check_same_thread=False) as conn:
            # ì„¤ì •ëœ ë¦¬ë¯¸íŠ¸ë§Œí¼ ìµœê·¼ migrated ë©”ì‹œì§€ë§Œ ì²˜ë¦¬
            target_messages = migrated_tool_messages[-limit:]
            for idx, msg in enumerate(target_messages, 1):
                try:
                    content_data = json.loads(msg.content)
                    result_id = content_data.get("result_id")
                    async with conn.cursor() as cursor:
                        await cursor.execute(
                            "SELECT content FROM tool_results_cache WHERE result_id = ? AND session_id = ?",
                            (result_id, session_id)
                        )
                        row = await cursor.fetchone()
                        if row:
                            full_content = json.loads(row[0])
                            chat_type = full_content.get("chat_type") or "unknown"
                            answer = full_content.get('answer', {})
                            if isinstance(answer, dict):
                                # ğŸš¨ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (í™”ì œ ì „í™˜ íŒë‹¨ ë° ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ ì •ë³´ ë³´ê°•)
                                doctors = []
                                for d in answer.get("doctors", []):
                                    name = d.get("name") or d.get("doctorname")
                                    if name:
                                        doctors.append({
                                            "name": name,
                                            "hospital": d.get("hospital") or d.get("shortname") or d.get("hospital_name"),
                                            "deptname": d.get("deptname"),
                                            "specialties": d.get("specialties")
                                        })
                                
                                hospitals = [h.get("shortname") or h.get("name") for h in answer.get("hospitals", []) if h.get("shortname") or h.get("name")]
                                
                                refined_context = {
                                    "historical_reference_type": chat_type,
                                    "entities_found_in_this_step": {
                                        "doctors": doctors,
                                        "hospitals": hospitals,
                                        "disease_context": answer.get("disease") or answer.get("standard_spec"),
                                        "department_context": answer.get("department")
                                    }
                                }
                                # ğŸš¨ [CRITICAL FIX] "migrated": Trueì™€ result_idë¥¼ ìœ ì§€í•˜ì—¬ ë¬´í•œ ì••ì¶• ë°©ì§€
                                msg.content = json.dumps({
                                    "migrated": True, 
                                    "is_historical_context": True,
                                    "result_id": result_id,
                                    "content_summary": content_data.get("summary"),
                                    "data": refined_context
                                }, ensure_ascii=False)
                                logger.info(f"âœ… [# {idx}] ê³¼ê±° ì»¨í…ìŠ¤íŠ¸ ë³µì› ì™„ë£Œ: {chat_type} (ì˜ì‚¬ {len(doctors)}ëª…)")
                                logger.info(f"   ã„´ [ì—”í‹°í‹°]: {refined_context['entities_found_in_this_step']}")
                            else:
                                logger.info(f"â„¹ï¸ [# {idx}] ë³µì› ìŠ¤í‚µ: ë°ì´í„° êµ¬ì¡° ë¶ˆì¼ì¹˜ ({chat_type})")
                except Exception as e:
                    logger.warning(f"âš ï¸ Proactive restoration failed for a message: {e}")
    # --- END: Proactive Refined Restoration ---

    # ğŸš¨ START: ì´ì „ í„´ì—ì„œ ë°œìƒí•œ ì—ëŸ¬ AIMessageë¥¼ ì œê±°í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í´ë¦°í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    cleaned_messages = []
    error_patterns = [
        "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì½˜í…ì¸  í•„í„°ë§ ì •ì±…ìœ¼ë¡œ ë‹µë³€ì´ ì¼ì‹œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ë°”ê¿” ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    ]
    
    last_human_idx = -1
    for i, msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            last_human_idx = i
            
    for i, msg in enumerate(messages):
        if i <= last_human_idx:
            cleaned_messages.append(msg)
        elif isinstance(msg, AIMessage):
            is_error_message = any(pattern in msg.content for pattern in error_patterns)
            if not is_error_message:
                cleaned_messages.append(msg)
            else:
                logger.info(f"Removed previous error AIMessage from context: {msg.content}")
        else:
            cleaned_messages.append(msg)
            
    state["messages"] = cleaned_messages
    messages = state["messages"]
    # ğŸš¨ END: ì—ëŸ¬ AIMessage ì œê±° ë¡œì§

    # --- START: Try-Catch-Retry & Cache Restoration Loop ---
    intermediate_messages = [] 
    try:
        logger.info("Calling model with original message...")
        response = await model.ainvoke(messages, config)
        
        # ğŸš¨ [NEW] ë‚´ë¶€ ìºì‹œ ë³µì› ë£¨í”„: LLMì´ get_cached_tool_resultë¥¼ í˜¸ì¶œí•˜ë©´ ì¦‰ì‹œ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ê³  ëª¨ë¸ì„ ì¬í˜¸ì¶œí•©ë‹ˆë‹¤.
        if isinstance(response, AIMessage) and response.tool_calls:
            cache_calls = [tc for tc in response.tool_calls if tc['name'] == 'get_cached_tool_result']
            if cache_calls:
                logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                logger.info(f"â”‚ ğŸ› ï¸  [INTERNAL_LOOP] ì¶”ê°€ ìºì‹œ ë³µì› ìš”ì²­ ê°ì§€ ({len(cache_calls)}ê±´) â”‚")
                logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
                intermediate_messages.append(response)
                
                loop_messages = list(messages)
                loop_messages.append(response)
                
                session_id = config["configurable"]["thread_id"]
                async with aiosqlite.connect(settings.sqlite_directory, check_same_thread=False) as conn:
                    for tc in cache_calls:
                        result_id = tc['args'].get('result_id')
                        if result_id:
                            async with conn.cursor() as cursor:
                                await cursor.execute(
                                    "SELECT content FROM tool_results_cache WHERE result_id = ? AND session_id = ?",
                                    (result_id, session_id)
                                )
                                row = await cursor.fetchone()
                                if row:
                                    logger.info(f"âœ… ìºì‹œ ë°ì´í„° ì›ë³¸ ë³µì› ì„±ê³µ (result_id: {result_id})")
                                    tool_msg = ToolMessage(content=row[0], tool_call_id=tc['id'])
                                    loop_messages.append(tool_msg)
                                    intermediate_messages.append(tool_msg)
                                else:
                                    logger.warning(f"âŒ ìºì‹œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (result_id: {result_id})")
                                    tool_msg = ToolMessage(content=json.dumps({"error": "Cache not found"}), tool_call_id=tc['id'])
                                    loop_messages.append(tool_msg)
                                    intermediate_messages.append(tool_msg)
                
                logger.info("ğŸ”„ ë³µì›ëœ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ ì¦‰ì‹œ ì¬í˜¸ì¶œ ì¤‘...")
                response = await model.ainvoke(loop_messages, config)
                logger.info("âœ¨ ëª¨ë¸ ì¬í˜¸ì¶œ ì™„ë£Œ.")
        # ğŸš¨ [END] ë‚´ë¶€ ìºì‹œ ë³µì› ë£¨í”„

    except Exception as e:
        logger.warning(f"LLM call failed with error: {e}. Checking for content filter.")
        error_message = str(e)
        
        if "An assistant message with 'tool_calls' must be followed by tool messages" in error_message or \
           "tool_call_ids did not have response messages" in error_message or \
           ("invalid_request_error" in error_message and "tool_calls" in error_message):
            
            logger.error(f"Detected invalid_request_error related to tool_calls.")
            last_ai_message_idx = -1
            for i in range(len(state["messages"]) - 1, -1, -1):
                if isinstance(state["messages"][i], AIMessage):
                    last_ai_message_idx = i
                    break
            
            if last_ai_message_idx != -1:
                state["messages"][last_ai_message_idx].tool_calls = []
                state["messages"][last_ai_message_idx].content = "ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            
            return {
                "messages": state["messages"],
                "retry": state.get("retry", 0),
                "location_history": state['location_history'],
                "entity_history": state['entity_history']
            }
        
        if "content management policy" in error_message or "content filter" in error_message:
            logger.warning("Original message filtered. Retrying with sanitized message.")
            original_user_message = next((msg.content for msg in reversed(state["messages"]) if isinstance(msg, HumanMessage)), "")
            sanitized_content = sanitize_prompt(original_user_message)
            
            sanitized_messages_for_llm = list(state["messages"])
            for i in range(len(sanitized_messages_for_llm) - 1, -1, -1):
                if isinstance(sanitized_messages_for_llm[i], HumanMessage):
                    sanitized_messages_for_llm[i] = HumanMessage(content=sanitized_content, id=sanitized_messages_for_llm[i].id)
                    break
            
            try:
                response = await model.ainvoke(sanitized_messages_for_llm, config)
            except Exception as retry_e:
                logger.error(f"Retry failed: {retry_e}")
                fallback_message = "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì½˜í…ì¸  í•„í„°ë§ ì •ì±…ìœ¼ë¡œ ë‹µë³€ì´ ì¼ì‹œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ë°”ê¿” ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                response = AIMessage(content=fallback_message)
        else:
            logger.error(f"Non-filter error: {e}")
            response = AIMessage(content="ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.")
    # --- END: Try-Catch-Retry & Cache Restoration Loop ---

    locale = state.get("locale") or "ko"
    greeting = LANGUAGE_GREETINGS.get(locale, DEFAULT_GREETING)
    if is_first_interaction_in_session and not response.content.strip().startswith(greeting):
        response.content = f"{greeting}\n\n {response.content}"

    state["last_ai_message"] = response.content 

    return {
        "messages": intermediate_messages + [response], 
        "retry": state.get("retry", 0),
        "location_history": state['location_history'],
        "entity_history": state['entity_history'],
        "last_ai_message": state["last_ai_message"]
    }



# 2ï¸âƒ£ ë¶„ê¸° ì—£ì§€
def should_continue(state: AgentState):
    """ëª¨ë¸ ë‹µë³€ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬í•˜ëŠ” edge"""
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "tools"
    return "validate"
    
# 3ï¸âƒ£ ê²€ì¦ ë…¸ë“œ
async def validate_node(state: AgentState) -> AgentState:
    """ì‘ë‹µì˜ ì ì ˆì„± ì—¬ë¶€ íŒë‹¨"""
    retry = state.get("retry", 0)

    if not settings.validation_enable or retry >= settings.validation_retry_limit:
        return {"messages": state["messages"], "retry": 0, "valid": True}

    answer = state["messages"][-1].content
    question = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            question = msg.content
            break

    prompt = VALIDATION_PROMPT.format(question=question, answer=answer)
    result = await llm.ainvoke(prompt)
    is_valid = result.content.strip().lower() == "yes"
       
    if is_valid:
        return {"messages": state["messages"], "retry": 0, "valid": True}
       
    new_messages = []
    reverse_messages = []
    found = False
    for msg in reversed(state["messages"]):
        if found:
            reverse_messages.append(msg)
        if not found and isinstance(msg, HumanMessage):
            reverse_messages.append(msg)
            found = True

    new_messages = list(reversed(reverse_messages))
    
    state["messages"].clear()
    state["messages"].extend(new_messages)
    
    return {"messages": state["messages"], "retry": retry + 1, "valid": False}

async def custom_tool_node(state: AgentState):
    """
    Custom tool node that intelligently routes calls to the appropriate tool for performance.
    It handles three types of location searches: user-centric, named location, and named location proximity.
    """
    tool_messages = []
    last_message = state["messages"][-1]
    
    locale = state.get("locale") or "ko"
    language_name = LANGUAGE_SET.get(locale, LANGUAGE_SET["ko"])

    latitude = state.get("latitude")
    longitude = state.get("longitude")

    original_query = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            original_query = msg.content
            break
    
    # --- NLP ê¸°ë°˜ ìœ„ì¹˜ ë¶„ì„: agent_nodeì—ì„œ ì´ë¯¸ ë¶„ì„/ì €ì¥í•œ location_history ê°’ì„ ì‚¬ìš© ---
    is_proximity_query = False
    classification = "NONE"
    anchor_noun = None
    
    location_history = state.get('location_history', [])
    if location_history:
        last_location = location_history[-1]
        if last_location.get('type') == 'GPS':
            is_proximity_query = True
            classification = "USER_LOCATION"
        elif last_location.get('type') == 'CONTEXTUAL':
            is_proximity_query = last_location.get('is_nearby', False)
            classification = "NAMED_LOCATION"
            # ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥ëœ ìœ„ì¹˜ ëª…ì‚¬ë¥¼ anchor_nounìœ¼ë¡œ ì‚¬ìš©
            anchor_noun = last_location.get('sigungu') or last_location.get('sido')
    # ---

    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        observation = None
        routed = False
        llm_legacy_list = tool_args.get('legacy_list')
        llm_proposal = tool_args.get('proposal')
        llm_request_recommend = tool_args.get('is_request_recommend')
        llm_request_distance = tool_args.get('llm_request_distance')
        logger.info(f"ll, select legacy_list : '{llm_legacy_list}'")
        logger.info(f"ll, select is_request_recommend : '{llm_request_recommend}'")
        logger.info(f"ll, select proposal : '{llm_proposal}'")
        logger.info(f"llm select llm_request_distance : '{llm_request_distance}'")
        logger.info(f"llm select tool_name : '{tool_name}'")
        # --- [PROXIMITY_INJECTION] ---
        # is_location_near íŒŒë¼ë¯¸í„°ë¥¼ ë°›ëŠ” íˆ´ì´ë¼ë©´, is_proximity_query ê°’ì„ ì£¼ì…
        if tool_name in [
            "search_hospitals_by_location_and_department",
            "search_doctors_by_location_and_department",
            "search_doctors_by_disease_and_location",
            "search_hospital_by_disease_and_location",
            "search_by_location_only"
        ]:
            # LLMì´ is_location_near ê°’ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª» ìƒì„±í•œ ê²½ìš°,
            # NLP ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°•ì œ ì£¼ì…/ìˆ˜ì •í•©ë‹ˆë‹¤.
            if tool_args.get('is_location_near') != is_proximity_query:
                 logger.info(f"Tool '{tool_name}'ì˜ is_location_near ê°’ì„ {is_proximity_query}(ìœ¼)ë¡œ ê°•ì œ ì£¼ì…/ìˆ˜ì •í•©ë‹ˆë‹¤.")
                 tool_args['is_location_near'] = is_proximity_query
        # --- [PROXIMITY_INJECTION_END] ---
        
        if tool_name == "search_doctor_for_else_question":
            # --- START of Comprehensive Routing Logic ---
            entities = await extract_entities_for_routing(llm_for_summary, state)
            loc = entities.get('location')
            dis = entities.get('disease')
            dep = entities.get('department')
            target = entities.get('target', 'ì˜ì‚¬')

            # ë¼ìš°íŒ…ì„ ìœ„í•œ ì§€ì—­(location) ê²°ì • ë¡œì§
            # 1. LLMì´ ìµœì‹  ëŒ€í™”ì—ì„œ ëª…ì‹œì ì¸ ì§€ì—­('loc')ì„ ì¶”ì¶œí–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            # 2. ë§Œì•½ ëª…ì‹œì ì¸ ì§€ì—­ì´ ì—†ë‹¤ë©´(if not loc), ì „ì²´ ëŒ€í™” ë§¥ë½ì—ì„œ íŒŒì•…ëœ ê¸°ì¤€ ëª…ì‚¬('anchor_noun')ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            #    ì´ë¥¼ í†µí•´ 'ê±°ê¸° ê·¼ì²˜'ì™€ ê°™ì€ ë§¥ë½ì  ì§ˆë¬¸ì— ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            if not loc and classification == "NAMED_LOCATION" and anchor_noun:
                logger.info(f"Explicit 'loc' not found. Falling back to anchor_noun from context: '{anchor_noun}'")
                loc = anchor_noun # NLPê°€ ì¶”ì¶œí•œ ëª…ì‚¬ë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©

            logger.info(f"is_proximity_query: '{is_proximity_query}' (classification: {classification})")

            # Case 1: User-centric proximity search (e.g., "near me + department")
            # 'loc'ì´ ì—†ê³ , 'dep' ë˜ëŠ” 'dis'ê°€ ìˆìœ¼ë©°, ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ì¼ ë•Œ
            if not loc and (dep or dis) and classification == "USER_LOCATION":
                params = {'latitude': latitude, 'longitude': longitude, 'is_location_near': True, 'limit': tool_args.get('limit')}
                if dep:
                    tool_key = 'search_doctors_by_location_and_department' if 'ì˜ì‚¬' in target else 'search_hospitals_by_location_and_department'
                    params['department'] = dep
                    try:
                        if tool_key == 'search_doctors_by_location_and_department':
                            params['proposal'] = llm_proposal or ""
                            observation = await search_doctors_by_location_and_department.ainvoke(params)
                        elif tool_key == 'search_hospitals_by_location_and_department':
                            observation = await search_hospitals_by_location_and_department.ainvoke(params)
                    except Exception as e:
                        observation = f"Error executing routed tool {tool_key}: {e}"
                    routed = True
                elif dis:
                    tool_key = 'search_doctors_by_disease_and_location' if 'ì˜ì‚¬' in target else 'search_hospital_by_disease_and_location'
                    params['disease'] = dis
                    try:
                        if tool_key == 'search_doctors_by_disease_and_location':
                            params['proposal'] = llm_proposal or ""
                            observation = await search_doctors_by_disease_and_location.ainvoke(params)
                        elif tool_key == 'search_hospital_by_disease_and_location':
                            observation = await search_hospital_by_disease_and_location.ainvoke(params)
                    except Exception as e:
                        observation = f"Error executing routed tool {tool_key}: {e}"
                    routed = True

            # Case 2.5: Named location, disease, and department search (e.g., "Ulsanì—ì„œ ë‹¹ë‡¨ë³‘ ì„±í˜•ì™¸ê³¼ ì˜í•˜ëŠ” ë³‘ì›")
            elif loc and dis and dep and target == 'ë³‘ì›':
                params = {'location': loc, 'disease': dis, 'department': dep, 'is_location_near': is_proximity_query, 'limit': tool_args.get('limit')}
                tool_key = 'search_hospital_by_disease_and_department'
                logger.info(f"Routing (Named Loc + Disease + Dept): {tool_key}")
                try:
                    observation = await search_hospital_by_disease_and_department.ainvoke(params)
                except Exception as e:
                    observation = f"Error executing routed tool {tool_key}: {e}"
                routed = True
            
            # ğŸš¨ [NEW] Case 2.6: Disease and department search without location for doctors
            elif dis and dep and not loc and 'ì˜ì‚¬' in target:
                params = {'disease': dis, 'department': dep, 'limit': tool_args.get('limit'), 'proposal': llm_proposal or ""}
                tool_key = 'search_doctors_by_disease_and_department'
                logger.info(f"Routing (Disease + Dept, No Loc): {tool_key}")
                try:
                    observation = await search_doctors_by_disease_and_department.ainvoke(params)
                except Exception as e:
                    observation = f"Error executing routed tool {tool_key}: {e}"
                routed = True
            # Case 2: Named location search (e.g., "in Ulsan + dept" or "near Ulsan + dept")
            elif loc and (dep or dis):
                # is_proximity_queryëŠ” NLP ë¶„ì„ ê²°ê³¼ë¡œ ì´ë¯¸ ê³„ì‚°ë¨
                params = {'location': loc, 'is_location_near': is_proximity_query, 'limit': tool_args.get('limit')}
                if dep:
                    tool_key = 'search_doctors_by_location_and_department' if 'ì˜ì‚¬' in target else 'search_hospitals_by_location_and_department'
                    params['department'] = dep
                    try:
                        if tool_key == 'search_doctors_by_location_and_department':
                            params['proposal'] = llm_proposal or ""
                            observation = await search_doctors_by_location_and_department.ainvoke(params)
                        elif tool_key == 'search_hospitals_by_location_and_department':
                            observation = await search_hospitals_by_location_and_department.ainvoke(params)
                    except Exception as e:
                        observation = f"Error executing routed tool {tool_key}: {e}"
                    routed = True
                elif dis:
                    tool_key = 'search_doctors_by_disease_and_location' if 'ì˜ì‚¬' in target else 'search_hospital_by_disease_and_location'
                    params['disease'] = dis
                    try:
                        if tool_key == 'search_doctors_by_disease_and_location':
                            params['proposal'] = llm_proposal or ""
                            observation = await search_doctors_by_disease_and_location.ainvoke(params)
                        elif tool_key == 'search_hospital_by_disease_and_location':
                            observation = await search_hospital_by_disease_and_location.ainvoke(params)
                    except Exception as e:
                        observation = f"Error executing routed tool {tool_key}: {e}"
                    routed = True

            # ğŸš¨ [NEW] Case 3: Location-only search
            elif loc and not dep and not dis:
                params = {
                    'location': loc,
                    'target': target,
                    'is_location_near': is_proximity_query,
                    'limit': tool_args.get('limit')
                }
                # 'ë‚´ ê·¼ì²˜'ì™€ ê°™ì€ ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ì‹œ, ëª…ì‹œì  ì§€ì—­ëª…ì´ ì—†ë”ë¼ë„ GPS ì¢Œí‘œë¥¼ ì‚¬ìš©
                if classification == "USER_LOCATION" and latitude is not None and longitude is not None:
                    params['latitude'] = latitude
                    params['longitude'] = longitude

                tool_key = 'search_by_location_only'
                params['proposal'] = llm_proposal or ""
                logger.info(f"Routing (Location Only): {tool_key} with params: {params}")
                try:
                    observation = await search_by_location_only.ainvoke(params)
                except Exception as e:
                    observation = f"Error executing routed tool {tool_key}: {e}"
                routed = True
            
            # ğŸš¨ [NEW] Case 4: disease-only search
            elif dis and not loc:
                if target == 'ë³‘ì›':
                    tool_key = 'search_hospital_by_disease'
                    params = {'disease': dis, 'limit': tool_args.get('limit')}
                    logger.info(f"Routing (Disease Only, Hospital): {tool_key} with params: {params}")
                    try:
                        observation = await search_hospital_by_disease.ainvoke(params)
                    except Exception as e:
                        observation = f"Error executing routed tool {tool_key}: {e}"
                    routed = True
            # ğŸš¨ [NEW] Case 5: Department-only search for Doctors
            elif dep and not loc and not dis and target == 'ì˜ì‚¬':
                params = {'department': dep, 'limit': tool_args.get('limit')}
                params['proposal'] = llm_proposal or ""
                tool_key = 'search_doctors_by_department_only'
                logger.info(f"Routing (Department Only, Doctors): {tool_key} with params: {params}")
                try:
                    observation = await search_doctors_by_department_only.ainvoke(params)
                except Exception as e:
                    observation = f"Error executing routed tool {tool_key}: {e}"
                routed = True
            # --- END of Comprehensive Routing Logic ---

        # ğŸš¨ NEW: Enrichment logic for search_doctor_by_hospital
        elif tool_name == "search_doctor_by_hospital":
            # Check if department is missing from the LLM's tool_args
            if not tool_args.get("deptname"): # Use 'deptname' as indicated by the user's log
                logger.info(f"Tool '{tool_name}' called without 'deptname'. Attempting to enrich from context.")
                context_dept = await extract_entities_for_routing_only_find_dept(llm_for_summary, state)
                if context_dept:
                    logger.info(f"Enriching '{tool_name}' call with department: '{context_dept}'")
                    tool_args["deptname"] = context_dept # Inject the department
            
            # Execute the (potentially enriched) search_doctor_by_hospital tool
            tool_to_call = tool_map.get(tool_name)
            if not tool_to_call:
                observation = f"Tool {tool_name} not found."
            else:
                try:
                    tool_args['proposal'] = llm_proposal or ""
                    observation = await tool_to_call.ainvoke(tool_args)
                except Exception as e:
                    observation = f"Error executing tool {tool_name}: {e}"
            routed = True # Mark as handled to skip the default execution below

        if not routed:
            tool_to_call = tool_map.get(tool_name)
            if not tool_to_call:
                observation = f"Tool {tool_name} not found."
            else:
                if tool_name == "search_doctor_for_else_question":
                    logger.info("No specific pattern matched. Falling back to search_doctor_for_else_question (SQL Agent).")
                    #language_instruction = f"\n\n[ì¤‘ìš”] ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ {language_name}(ìœ¼)ë¡œ, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
                    #if language_instruction not in tool_args.get("question", ""):
                    #     tool_args["question"] += language_instruction
                    
                    # NLP ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ GPS ì¢Œí‘œ ì „ë‹¬
                    if classification == "USER_LOCATION":
                        if latitude is not None: tool_args["latitude"] = latitude
                        if longitude is not None: tool_args["longitude"] = longitude
                    # NAMED_LOCATION ì´ë‚˜ NONE ì˜ ê²½ìš°, SQL Agentê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•˜ë„ë¡ ìœ„ì„ (ì¢Œí‘œ ì „ë‹¬ ì•ˆí•¨)
                    else:
                        tool_args.pop("latitude", None)
                        tool_args.pop("longitude", None)
                    tool_args['proposal'] = llm_proposal or ""

                elif tool_name == "recommend_hospital":
                    if latitude is not None: tool_args['latitude'] = latitude
                    if longitude is not None: tool_args['longitude'] = longitude
                    tool_args['is_nearby'] = is_proximity_query
                elif tool_name == "recommand_doctor":
                    # recommand_doctorëŠ” 'disease'ê°€ í•„ìˆ˜. 'disease'ê°€ ì—†ìœ¼ë©´ entity_history ê¸°ë°˜ ë¼ìš°íŒ… ì‹œë„
                    if not tool_args.get('disease'):
                        logger.info(f"Tool '{tool_name}' called without 'disease'. Attempting to route using entity_history and current context.")
                        
                        # entity_historyì—ì„œ ìµœì‹  ì—”í‹°í‹° ì •ë³´ ì¶”ì¶œ
                        entities_from_history = await extract_entities_for_routing(llm_for_summary, state)
                        loc_from_history = entities_from_history.get('location')
                        dep_from_history = entities_from_history.get('department')
                        dis_from_history = entities_from_history.get('diseases') # ì§ˆí™˜ëª…ë„ ê°€ì ¸ì˜´ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
                        target_from_history = entities_from_history.get('target', 'ì˜ì‚¬')

                        # ë¼ìš°íŒ…ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
                        fallback_tool_key = None
                        fallback_params = {}

                        # ë¼ìš°íŒ… ìš°ì„ ìˆœìœ„: department -> disease -> location
                        # 1. entity_historyì— departmentê°€ ìˆëŠ” ê²½ìš°
                        if dep_from_history:
                            logger.info(f"[recommand_doctor Fallback] Department found in history: {dep_from_history}. Routing to department-based search.")
                            fallback_params = {'department': dep_from_history, 'limit': tool_args.get('limit', 10), 'proposal': llm_proposal or ""}
                            
                            # ìœ„ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰, ì—†ìœ¼ë©´ ì§„ë£Œê³¼ë§Œìœ¼ë¡œ ê²€ìƒ‰
                            has_location_info = (latitude is not None and longitude is not None) or (loc_from_history and loc_from_history.strip())
                            if has_location_info:
                                fallback_tool_key = 'search_doctors_by_location_and_department'
                                if latitude is not None: fallback_params['latitude'] = latitude
                                if longitude is not None: fallback_params['longitude'] = longitude
                                if loc_from_history: fallback_params['location'] = loc_from_history
                                fallback_params['is_location_near'] = is_proximity_query # is_proximity_queryëŠ” custom_tool_node ì‹œì‘ ë¶€ë¶„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨
                            else:
                                fallback_tool_key = 'search_doctors_by_department_only'
                        
                        # 2. departmentëŠ” ì—†ê³  diseaseê°€ entity_historyì— ìˆëŠ” ê²½ìš°
                        elif dis_from_history:
                            logger.info(f"[recommand_doctor Fallback] Disease found in history: {dis_from_history}. Routing to disease-based search.")
                            # 'ì˜ì‚¬' íƒ€ê²Ÿì´ë¯€ë¡œ search_doctors_by_disease_and_location ì‚¬ìš©
                            fallback_tool_key = 'search_doctors_by_disease_and_location'
                            fallback_params = {'disease': dis_from_history, 'limit': tool_args.get('limit', 10), 'proposal': llm_proposal or ""}
                            
                            has_location_info = (latitude is not None and longitude is not None) or (loc_from_history and loc_from_history.strip())
                            if has_location_info:
                                if latitude is not None: fallback_params['latitude'] = latitude
                                if longitude is not None: fallback_params['longitude'] = longitude
                                if loc_from_history: fallback_params['location'] = loc_from_history
                                fallback_params['is_location_near'] = is_proximity_query
                            # ì´ ê²½ìš° diseaseë§Œìœ¼ë¡œëŠ” ì§„ë£Œê³¼ ì¶”ë¡ ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ì¼ë‹¨ ì§ì ‘ í˜¸ì¶œ ì‹œë„
                        
                        # 3. department, diseaseëŠ” ì—†ê³  locationë§Œ entity_historyì— ìˆëŠ” ê²½ìš°
                        elif loc_from_history:
                            logger.info(f"[recommand_doctor Fallback] Only Location found in history: {loc_from_history}. Routing to search_by_location_only.")
                            fallback_tool_key = 'search_by_location_only'
                            fallback_params = {
                                'location': loc_from_history,
                                'target': target_from_history,
                                'is_location_near': is_proximity_query,
                                'limit': tool_args.get('limit'),
                                'proposal': llm_proposal or ""
                            }
                            if classification == "USER_LOCATION" and latitude is not None and longitude is not None:
                                fallback_params['latitude'] = latitude
                                fallback_params['longitude'] = longitude

                        if fallback_tool_key:
                            try:
                                # department/disease ì¸ìê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •
                                if 'department' in fallback_params and isinstance(fallback_params['department'], list):
                                    fallback_params['department'] = fallback_params['department'][0] if fallback_params['department'] else None
                                if 'disease' in fallback_params and isinstance(fallback_params['disease'], list):
                                    fallback_params['disease'] = fallback_params['disease'][0] if fallback_params['disease'] else None

                                cleaned_fallback_params = {k: v for k, v in fallback_params.items() if v is not None}
                                logger.info(f"[recommand_doctor Fallback] Executing fallback tool '{fallback_tool_key}' with params: {cleaned_fallback_params}")
                                
                                # ë¼ìš°íŒ…ëœ ë„êµ¬ í˜¸ì¶œ
                                if fallback_tool_key == 'search_doctors_by_location_and_department':
                                    observation = await search_doctors_by_location_and_department.ainvoke(cleaned_fallback_params)
                                elif fallback_tool_key == 'search_doctors_by_department_only':
                                    observation = await search_doctors_by_department_only.ainvoke(cleaned_fallback_params)
                                elif fallback_tool_key == 'search_doctors_by_disease_and_location':
                                    observation = await search_doctors_by_disease_and_location.ainvoke(cleaned_fallback_params)
                                elif fallback_tool_key == 'search_by_location_only':
                                    observation = await search_by_location_only.ainvoke(cleaned_fallback_params)
                                # í•„ìš”í•œ ê²½ìš° ë‹¤ë¥¸ ë„êµ¬ë„ ì—¬ê¸°ì— ì¶”ê°€
                                
                                routed = True
                            except Exception as e:
                                logger.error(f"Error executing routed fallback tool {fallback_tool_key}: {e}", exc_info=True)
                                observation = {"chat_type": "general", "answer": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
                                routed = True # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¼ìš°íŒ…ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê¸°ë³¸ í˜¸ì¶œ ë°©ì§€

                    if not routed: # Fallback ë¼ìš°íŒ…ì´ ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì›ë˜ recommand_doctor í˜¸ì¶œ (diseaseê°€ ì—¬ì „íˆ ì—†ìŒ)
                        # ì´ ì‹œì ì—ì„œëŠ” diseaseê°€ ì—¬ì „íˆ ì—†ìœ¼ë¯€ë¡œ, recommand_doctorê°€ ì§ˆí™˜ëª…ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•  ê²ƒì„.
                        # ì´ëŠ” ê¸°ì¡´ ë™ì‘ì´ë¯€ë¡œ ìœ ì§€.
                        if latitude is not None: tool_args['latitude'] = latitude
                        if longitude is not None: tool_args['longitude'] = longitude
                        tool_args['proposal'] = llm_proposal or ""


                
                try:
                    # ì—¬ê¸°ì— ì¶”ê°€
                    if tool_name == "search_doctor_for_else_question":
                        tool_args["use_json_output"] = True 

                    observation = await tool_to_call.ainvoke(tool_args)
                except Exception as e:
                    observation = f"Error executing tool {tool_name}: {e}"

        # ğŸš¨ Add this line to define is_empty_result
        is_empty_result = is_result_empty(tool_name, observation) # tool_nameê³¼ observationì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ê°€ ë¹„ì—ˆëŠ”ì§€ í™•ì¸
        logger.info(f"is_empty_result {is_empty_result} ê°’ì—¬ë¶€ì— ë”°ë¼ íŒë‹¨ í•„ìš”")
        if observation and isinstance(observation, dict):
            # ğŸ’¡ front_sort_type ì£¼ì… ë¡œì§
            if 'answer' in observation and isinstance(observation['answer'], dict):
                if tool_name == "recommand_doctor":
                    observation['answer']['front_sort_type'] = "evaluation"
                    logger.info(f"Injected 'front_sort_type': 'evaluation' for recommand_doctor tool.")
                else:
                    observation['answer']['front_sort_type'] = "distance" if is_proximity_query else "evaluation"
                    logger.info(f"Injected 'front_sort_type': '{observation['answer']['front_sort_type']}' based on proximity query flag.")
            if llm_request_distance == 'True':
                     observation['answer']['front_sort_type'] = 'distance'
            effective_tool_name = observation.get('chat_type', tool_name)

            if is_empty_result: # ì´ê³³ì—ì„œ is_empty_resultê°€ ì‚¬ìš©ë¨
                logger.info(f"Tool {effective_tool_name} returned empty. Attempting fallback with department-based search.")
                
                # 1. ì§ˆë³‘ëª… ì¶”ì¶œ (ì´ë¯¸ custom_tool_node ì‹œì‘ ë¶€ë¶„ì—ì„œ ì¶”ì¶œëœ entities ì‚¬ìš©)
                entities = await extract_entities_for_routing(llm_for_summary, state) # ì¬í˜¸ì¶œí•˜ì—¬ ìµœì‹  ìƒíƒœ ë°˜ì˜
                loc = entities.get('location') # UnboundLocalError ë°©ì§€ë¥¼ ìœ„í•´ loc ë³€ìˆ˜ ì¬í• ë‹¹

                dis = entities.get('disease')
                dep = entities.get('department') # ê¸°ì¡´ ì¶”ì¶œëœ departmentë„ í™œìš©
                target = entities.get('target', 'ì˜ì‚¬')
                
                logger.info(f"Tool {effective_tool_name} returned empty. Attempting fallback with department-based search.")
                if dis: # <-- ì¡°ê±´ ë³€ê²½
                    
                    inferred_depts = [] # inferred_deptsë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    
                    if dep: # ê¸°ì¡´ì— ì¶”ì¶œëœ departmentê°€ ìˆë‹¤ë©´ ìš°ì„  ì‚¬ìš©
                        inferred_depts.append(dep)
                        logger.info(f"Using pre-extracted department for fallback: '{dep}'")
                    
                    if not inferred_depts and dis: # departmentê°€ ì—†ê³ , diseaseê°€ ìˆë‹¤ë©´ LLMìœ¼ë¡œ ì¶”ë¡  ì‹œë„
                        dept_inference_prompt = f"ì§ˆë³‘ '{dis}'ì— ëŒ€í•´ ì¼ë°˜ì ìœ¼ë¡œ ì§„ë£Œí•˜ëŠ” ì§„ë£Œê³¼ëª©ì„ 2-3ê°€ì§€ ì •ë„ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì•Œë ¤ì¤˜. ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ì–´. (ì˜ˆ: ['ê°ì—¼ë‚´ê³¼', 'ê°€ì •ì˜í•™ê³¼'])"
                        
                        try:
                            llm_response = await llm_for_summary.ainvoke(dept_inference_prompt)
                            json_match = re.search(r"```json\n(.*?)\n```", llm_response.content, re.DOTALL)
                            if json_match:
                                json_str = json_match.group(1)
                                inferred_depts_raw = json.loads(json_str)
                            else:
                                inferred_depts_raw = json.loads(llm_response.content)
                                
                            if isinstance(inferred_depts_raw, list):
                                inferred_depts.extend(inferred_depts_raw)
                            elif isinstance(inferred_depts_raw, str):
                                inferred_depts.append(inferred_depts_raw)
                            logger.info(f"Inferred departments for '{dis}': {inferred_depts}")

                        except (json.JSONDecodeError, ValueError) as e:
                            logger.error(f"Failed to infer departments for disease '{dis}': {e}", exc_info=True)
                    
                    if inferred_depts: # ì§„ë£Œê³¼ëª©ì´ ì¶”ë¡ ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ depì—ì„œ í™•ë³´ë˜ì—ˆë‹¤ë©´ ë¼ìš°íŒ… ì‹œì‘
                        new_observation = None
                        fallback_tool_key = None
                        fallback_params = {}

                        # ìœ„ì¹˜ ì •ë³´ í™•ì¸
                        has_location_info = (latitude is not None and longitude is not None) or (loc and loc.strip())
                        
                        if has_location_info:
                            # ì§€ì—­ ì •ë³´ ìˆìŒ (ì˜ì‚¬/ë³‘ì› ê²€ìƒ‰)
                            if target == 'ì˜ì‚¬':
                                fallback_tool_key = 'search_doctors_by_location_and_department'
                                fallback_params = {
                                    'department': inferred_depts,
                                    'is_location_near': is_proximity_query,
                                    'latitude': latitude,
                                    'longitude': longitude,
                                    'location': loc,
                                    'proposal' : llm_proposal or ""
                                }
                            else: # target == 'ë³‘ì›'
                                fallback_tool_key = 'search_hospitals_by_location_and_department'
                                fallback_params = {
                                    'department': inferred_depts,
                                    'is_location_near': is_proximity_query,
                                    'latitude': latitude,
                                    'longitude': longitude,
                                    'location': loc
                                }
                        else:
                            # ì§€ì—­ ì •ë³´ ì—†ìŒ (ì˜ì‚¬/ë³‘ì› ê²€ìƒ‰)
                            if target == 'ì˜ì‚¬':
                                fallback_tool_key = 'search_doctors_by_department_only'
                                fallback_params = {
                                    'department': inferred_depts,
                                    'proposal' : llm_proposal or ""
                                }
                            else: # target == 'ë³‘ì›'
                                fallback_tool_key = 'recommend_hospital'
                                fallback_params = {
                                    'department': inferred_depts
                                }
                       
                        if fallback_tool_key:
                            try:
                                cleaned_fallback_params = {k: v for k, v in fallback_params.items() if v is not None}
                                # department ì¸ìê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                                if (fallback_tool_key == 'search_hospitals_by_location_and_department' or
                                    fallback_tool_key == 'search_doctors_by_location_and_department') and \
                                   isinstance(cleaned_fallback_params.get('department'), list) and \
                                   len(cleaned_fallback_params['department']) > 0:
                                    cleaned_fallback_params['department'] = cleaned_fallback_params['department'][0]
                                elif fallback_tool_key == 'recommend_hospital' and isinstance(cleaned_fallback_params.get('department'), str):
                                    cleaned_fallback_params['department'] = [cleaned_fallback_params['department']]

                                new_observation = None
                                try:
                                    if fallback_tool_key == 'search_doctors_by_location_and_department':
                                        cleaned_fallback_params['proposal'] = llm_proposal or ""
                                        new_observation = await search_doctors_by_location_and_department.ainvoke(cleaned_fallback_params)
                                    elif fallback_tool_key == 'search_hospitals_by_location_and_department':
                                        new_observation = await search_hospitals_by_location_and_department.ainvoke(cleaned_fallback_params)
                                    elif fallback_tool_key == 'search_doctors_by_department_only':
                                        cleaned_fallback_params['proposal'] = llm_proposal or ""
                                        new_observation = await search_doctors_by_department_only.ainvoke(cleaned_fallback_params)
                                    elif fallback_tool_key == 'recommend_hospital':
                                        new_observation = await recommend_hospital.ainvoke(cleaned_fallback_params)
                                    else:
                                        # ì˜ˆì™¸ ì²˜ë¦¬: ì˜ˆìƒì¹˜ ëª»í•œ fallback_tool_keyê°€ ë°œìƒí–ˆì„ ê²½ìš°
                                        raise ValueError(f"Unexpected fallback tool key: {fallback_tool_key}")
                                except Exception as tool_e:
                                    logger.info(f"[Fallback Logic] Error during invocation of fallback tool '{fallback_tool_key}' with params {cleaned_fallback_params}: {tool_e}", exc_info=True)
                                    new_observation = {"chat_type": "general", "answer": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(tool_e)}"}

                                if new_observation is None or 'answer' not in new_observation or is_result_empty(fallback_tool_key, new_observation):
                                    logger.info(f"[Fallback Logic] Second fallback also returned empty. Returning generic info message.")
                                    observation = {"chat_type": "general", "answer": "ì•„ì‰½ê²Œë„ ìš”ì²­í•˜ì‹  ì •ë³´ëŠ” í˜„ì¬ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ì¢€ ë” êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë³‘ì› ì´ë¦„, ì§„ë£Œ ê³¼ëª©)ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìì„¸íˆ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."}
                                else:
                                    logger.info(f"[Fallback Logic] Second fallback returned results. Using new_observation.")
                                    observation = new_observation

                            except Exception as e:
                                logger.info(f"[Fallback except Exception else] 865í–‰ ")
                                observation = {"chat_type": "general", "answer": f"ë„êµ¬ í´ë°± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
                        else:
                            logger.info(f"[Fallback except Exception else] 868í–‰ ")
                            observation = {"chat_type": "general", "answer": "ì•„ì‰½ê²Œë„ ìš”ì²­í•˜ì‹  ì •ë³´ëŠ” í˜„ì¬ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ì¢€ ë” êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë³‘ì› ì´ë¦„, ì§„ë£Œ ê³¼ëª©)ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìì„¸íˆ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."}
                    else:
                        # ì§„ë£Œê³¼ëª© ì¶”ë¡ ì— ì‹¤íŒ¨í•˜ê±°ë‚˜ depë„ ì—†ëŠ” ê²½ìš°
                        logger.info(f"[Fallback except Exception else] 872í–‰ ")
                        observation = {"chat_type": "info", "answer": "ì•„ì‰½ê²Œë„ ìš”ì²­í•˜ì‹  ì •ë³´ëŠ” í˜„ì¬ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ì¢€ ë” êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë³‘ì› ì´ë¦„, ì§„ë£Œ ê³¼ëª©)ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìì„¸íˆ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."}

                else: # (dis or dep) ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                    if not observation.get('answer'):
                        logger.info(f"[Fallback except Exception else] 877í–‰ ")
                        observation = {"chat_type": "general", "answer": "ì•„ì‰½ê²Œë„ ìš”ì²­í•˜ì‹  ì •ë³´ëŠ” í˜„ì¬ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ì¢€ ë” êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ë³‘ì› ì´ë¦„, ì§„ë£Œ ê³¼ëª©)ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìì„¸íˆ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."}

        
        # ğŸš¨ ì´ ë¶€ë¶„ì—ì„œ observationì´ Noneì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  í•­ìƒ dict í˜•íƒœë¡œ ë§Œë“¦
        if observation is None:
            observation = {"chat_type": "general", "answer": f"Tool {tool_name} not found or failed to execute, but no specific error message was captured."}
        elif not isinstance(observation, dict):
            # observationì´ ë¬¸ìì—´ ë“±ì˜ dictê°€ ì•„ë‹Œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë˜í•‘
            observation = {"chat_type": "general", "answer": str(observation)}

        tool_messages.append(
            ToolMessage(content=json.dumps(observation, ensure_ascii=False), tool_call_id=tool_call["id"])
        )
        
    return {"messages": tool_messages}

# 4ï¸âƒ£ ì¬ì‹œë„ ë¶„ê¸° ì²˜ë¦¬ í•¨ìˆ˜
def should_retry(state: AgentState) -> Literal["agent", END]:
    """ê²€ì¦ì´í›„ì— ë¶„ê¸°í•˜ëŠ” edge"""
    if state["valid"]:
        return END
    else:
        logger.info(f"Retry attempt {state.get('retry', 0)}")
        return "agent"
    
# ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•˜ëŠ” í•¨ìˆ˜
async def get_compiled_graph():
    workflow = StateGraph(AgentState)

    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", custom_tool_node)
    workflow.add_node("validate", validate_node)

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", should_continue)
    workflow.add_edge("tools", "agent")
    workflow.add_conditional_edges("validate", should_retry, {"agent": "agent", END: END})

    conn = await aiosqlite.connect(settings.sqlite_directory, check_same_thread=False)
    
    # ğŸš¨ Add this block to create tool_results_cache table if it doesn't exist
    async with conn.cursor() as cursor:
        await cursor.execute("""
            CREATE TABLE IF NOT EXISTS tool_results_cache (
                session_id TEXT NOT NULL,
                result_id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """
        )
        await conn.commit()
    # ğŸš¨ End of new block
    
    memory = AsyncSqliteSaver(conn=conn)

    graph = workflow.compile(checkpointer=memory)
    return graph