async def agent_node(state: AgentState, config: RunnableConfig):
    """ëª¨ë¸ì„ í†µí•´ ë‹µë³€í•˜ëŠ” Agent ë…¸ë“œ. ì½˜í…ì¸  í•„í„° ì—ëŸ¬ ë°œìƒ ì‹œ, ë©”ì‹œì§€ë¥¼ ì •ì œí•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤."""
    
    # ğŸš¨ ì¶”ê°€: last_ai_messageê°€ stateì— ì—†ì„ ê²½ìš° ì´ˆê¸°í™”
    if "last_ai_message" not in state:
        state["last_ai_message"] = None
    
    current_user_message = ""
    if state["messages"] and isinstance(state["messages"][-1], HumanMessage):
        current_user_message = state["messages"][-1].content

    num_human_messages = len([msg for msg in state['messages'] if isinstance(msg, HumanMessage)])
    is_first_interaction_in_session = (num_human_messages == 1)
    locale = state.get("locale") or "ko"

    # --- START: ì´ˆê¸° ìš”ì²­ í†µí•© ì²˜ë¦¬ (í˜„ì¬ ìœ„ì¹˜, ì‘ê¸‰ ìƒí™©, ê¸ˆì§€ëœ ì¶”ì²œ) ---
    response = await classify_and_handle_initial_requests(state, config, current_user_message, is_first_interaction_in_session, locale)
    if response:
        return response
    # --- END: ì´ˆê¸° ìš”ì²­ í†µí•© ì²˜ë¦¬ ---

    # --- Start of New Location Context Management ---
    
    # 1. Call the new location context manager
    updated_history, clarification_question = await update_location_context(
        llm=llm_for_summary, # Pass the llm instance
        user_message=current_user_message,
        location_history=state.get('location_history', []),
        latitude=state.get('latitude'),
        longitude=state.get('longitude')
    )

    # 2. Update state with the new history
    state['location_history'] = updated_history

    # ì—”íŠ¸ë¦¬ ì €ì¥ ì—¬ë¶€  ê°ì§€ (í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì‹œ)
    # ğŸš¨ START: AIì˜ ìµœì¢… ë‹µë³€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œí•˜ì—¬ entity_history ì—…ë°ì´íŠ¸
    current_entity_history = state.get("entity_history")
    if current_entity_history is None:
        current_entity_history = {"hospitals": [], "doctors": [], "departments": [], "diseases": [], "location": None}
    
    # state["last_ai_message"]ê°€ ì¡´ì¬í•  ë•Œë§Œ ì—”í‹°í‹° ì¶”ì¶œ ë¡œì§ ì‹¤í–‰
    # --- START: ì§ì „ AIMessage ë¶„ì„ì„ í†µí•œ ë ˆê±°ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    ai_message_recommendation_info = "" # ìµœì¢… system promptì— ì‚½ì…ë  ì •ë³´
    last_ai_message_content = None # ì§ì „ AIMessageì˜ contentë¥¼ ì €ì¥í•  ë³€ìˆ˜
    
    # ë§ˆì§€ë§‰ HumanMessageì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    last_human_message_idx = -1
    for i in range(len( state["messages"]) - 1, -1, -1):
        if isinstance( state["messages"][i], HumanMessage):
            last_human_message_idx = i
            break

    # ë§ˆì§€ë§‰ HumanMessage ì´ì „ì— AIMessageê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    # ì¦‰, last_human_message_idx - 1 ìœ„ì¹˜ì˜ ë©”ì‹œì§€ê°€ AIMessageì—¬ì•¼ í•©ë‹ˆë‹¤.
    if last_human_message_idx > 0 and isinstance( state["messages"][last_human_message_idx - 1], AIMessage):
        last_ai_message_content =  state["messages"][last_human_message_idx - 1].content
    # --- END: ì§ì „ AIMessage ë¶„ì„ì„ í†µí•œ ë ˆê±°ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ---

    last_ai_message_to_process = last_ai_message_content
    if last_ai_message_to_process:
        state["entity_history"] = await extract_entities_from_ai_response_and_update_history(
            llm=llm_for_summary,
            ai_response_content=last_ai_message_to_process,
            current_entity_history=current_entity_history
        )
    else:
        state["entity_history"] = current_entity_history # last_ai_messageê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ entity_history ìœ ì§€ ë˜ëŠ” ì´ˆê¸°í™”
    # ğŸš¨ END: AIì˜ ìµœì¢… ë‹µë³€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œí•˜ì—¬ entity_history ì—…ë°ì´íŠ¸
    # --- End of New Entity Context Management ---

    # 3. If the manager returned a question, ask it immediately.
    if clarification_question:
        greeting = LANGUAGE_GREETINGS.get(locale, DEFAULT_GREETING)
        if is_first_interaction_in_session:
            clarification_question = f"{greeting}

  {clarification_question}"
        logger.info(f"Asking clarification question from location manager: {clarification_question}")
        response = AIMessage(content=clarification_question)
        return {
            "messages": [response], 
            "retry": state.get("retry", 0), 
            "valid": True, 
            "location_history": state['location_history'],
            "entity_history": state['entity_history']
        }

    # 5. Prepare entity information to be injected into the main prompt as a structured JSON block
    persistent_facts_info = ""
    # ğŸš¨ ìˆ˜ì •: state['entity_history']ê°€ ì¡´ì¬í•˜ê³  ë‚´ìš©ì´ ìˆì„ ê²½ìš° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
    if state.get("entity_history") and any(state["entity_history"].values()):
        persistent_facts_info += f"""
/*
IMPORTANT: The following JSON block contains the latest confirmed entities from the conversation history.
The AI's previous message contained the following recommendations or suggestions. The user's current message might be an acceptance or follow-up on these. Prioritize the following information if the user's intent aligns with these details.
Inherited entities:
*/
{json.dumps(state["entity_history"], ensure_ascii=False, indent=2)}
"""
    
    messages = state["messages"] 

    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ì •ë³´ ì¤€ë¹„
    current_latitude = state.get('latitude')
    current_longitude = state.get('longitude')
    location_gps_info = ""
    if current_latitude is not None and current_longitude is not None:
        location_gps_info = f"

/*
IMPORTANT: User's current GPS location is available.
Latitude: {current_latitude}
Longitude: {current_longitude}
Prioritize using location-based tools if the user asks for nearby facilities.
*/
"
    
    locale = state.get("locale") or "ko"
    language_name = LANGUAGE_SET.get(locale, LANGUAGE_SET["ko"])
    language_rule = f"

**Response Language Rule**
- The AI counselor's final response MUST be generated in **{language_name}**.
"

    # 2. ê¸°ë³¸ SYSTEM_PROMPTì—ì„œ ê¸°ì¡´ GPS ì •ë³´ ë¸”ë¡ ë° ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
    clean_system_prompt_base = re.sub(r'\[ì‚¬ìš©ì í˜„ì¬ ìœ„ì¹˜ ì •ë³´ \(GPS\)\].*?\[ì§€ì—­ ì‚¬ì „ ë¶„ì„ í”Œë˜ê·¸\]', '', SYSTEM_PROMPT, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'
+/\*.*?IMPORTANT:.*?\*/
*', '', clean_system_prompt_base, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'\[ì§€ì—­ ì‚¬ì „ ë¶„ì„ í”Œë˜ê·¸\].*?---', '', clean_system_prompt_base, flags=re.DOTALL)
    clean_system_prompt_base = re.sub(r'\[Location Context\].*?\}', '', clean_system_prompt_base, flags=re.DOTALL)

    # 3. ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    final_system_prompt_content = clean_system_prompt_base + persistent_facts_info + location_gps_info + language_rule
    
    # 4. messages ë¦¬ìŠ¤íŠ¸ì—ì„œ ê¸°ì¡´ SystemMessageë¥¼ ëª¨ë‘ ì œê±°
    messages[:] = [msg for msg in messages if not isinstance(msg, SystemMessage)]
    
    # 5. ìµœì¢… êµ¬ì„±ëœ SystemMessageë¥¼ ë§¨ ì•ì— ì‚½ì…
    messages.insert(0, SystemMessage(content=final_system_prompt_content))

    # --- START: ToolMessage ë§ˆì´ê·¸ë ˆì´ì…˜ (ì••ì¶• ë° ìºì‹±) ---
    # ëŒ€ìš©ëŸ‰ ToolMessageë¥¼ SQLiteì— ì €ì¥í•˜ê³  ìš”ì•½ ì •ë³´ë¡œ ëŒ€ì²´í•˜ì—¬ í† í°ì„ ì ˆì•½í•©ë‹ˆë‹¤.
    if settings.llm_summary_verbose:
        session_id = config["configurable"]["thread_id"]
        async with aiosqlite.connect(settings.sqlite_directory, check_same_thread=False) as conn:
            # í˜„ì¬ í„´ì—ì„œ ë°©ê¸ˆ ì‹¤í–‰ëœ ìµœì‹  ToolMessageëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ ì œì™¸í•œë‹¤.
            latest_tool_messages_indices = set()
            if len(messages) > 1 and isinstance(messages[-1], ToolMessage):
                if isinstance(messages[-2], AIMessage) and messages[-2].tool_calls:
                    num_tool_calls = len(messages[-2].tool_calls)
                    for i in range(num_tool_calls):
                        idx_to_exclude = len(messages) - 1 - i
                        if idx_to_exclude >= 0 and isinstance(messages[idx_to_exclude], ToolMessage):
                             latest_tool_messages_indices.add(idx_to_exclude)
                        else:
                            break

            # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° 'ê³¼ê±°ì˜' ToolMessageë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜
            for i in range(len(messages) - 1, -1, -1):
                if i in latest_tool_messages_indices:
                    continue
                msg = messages[i]
                if isinstance(msg, ToolMessage):
                    try:
                        tool_content_json = json.loads(msg.content)
                        if not isinstance(tool_content_json, dict) or tool_content_json.get("migrated") is True:
                            continue
                        
                        original_content = msg.content
                        result_id = str(uuid.uuid4())
                        await conn.execute(
                            "INSERT OR REPLACE INTO tool_results_cache (session_id, result_id, content) VALUES (?, ?, ?)",
                            (session_id, result_id, original_content)
                        )
                        await conn.commit()
                        
                        placeholder_summary = "ê³¼ê±° ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ì™¸ë¶€ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
                        param_dict = {}
                        if 'chat_type' in tool_content_json:
                            answer_content = tool_content_json.get('answer')
                            if isinstance(answer_content, dict):
                                summary_parts = []
                                count_info = ""
                                if answer_content.get('disease'):
                                    summary_parts.append(f"ì§ˆí™˜: {answer_content['disease']}")
                                    param_dict['disease'] = answer_content['disease']
                                if answer_content.get('department'):
                                    summary_parts.append(f"ì§„ë£Œê³¼: {answer_content['department']}")
                                    param_dict['department'] = answer_content['department']
                                if answer_content.get('hospital'):
                                    summary_parts.append(f"ë³‘ì›: {answer_content['hospital']}")
                                    param_dict['hospital'] = answer_content['hospital']
                                elif answer_content.get('hospitals') and len(answer_content['hospitals']) > 0:
                                    first_hosp = answer_content['hospitals'][0].get('name', '')
                                    if first_hosp: summary_parts.append(f"ì£¼ìš” ë³‘ì›: {first_hosp}")
                                    param_dict['hospital'] = first_hosp
                                    param_dict['hospital_count'] = len(answer_content['hospitals'])
                                if answer_content.get('doctors') and len(answer_content['doctors']) > 0:
                                    first_doc = answer_content['doctors'][0].get('name', '')
                                    if first_doc: summary_parts.append(f"ì£¼ìš” ì˜ì‚¬: {first_doc}")
                                    param_dict['doctor'] = first_doc
                                    param_dict['doctor_count'] = len(answer_content['doctors'])
                                
                                if answer_content.get('doctors'): count_info = f"{len(answer_content['doctors'])}ëª…ì˜ ì˜ì‚¬ ì •ë³´"
                                elif answer_content.get('hospitals'): count_info = f"{len(answer_content['hospitals'])}ê°œì˜ ë³‘ì› ì •ë³´"

                                if summary_parts or count_info:
                                    placeholder_summary = f"ê³¼ê±° {tool_content_json['chat_type']} ê²°ê³¼: {count_info}{' (' + ', '.join(summary_parts) + ')' if summary_parts else ''}"
                            elif isinstance(answer_content, str):
                                placeholder_summary = f"ê³¼ê±° {tool_content_json['chat_type']} ê²°ê³¼: {answer_content[:100]}... (ì €ì¥ë¨)"

                        msg.content = json.dumps({
                            "migrated": True,
                            "result_id": result_id,
                            "summary": placeholder_summary,
                            "param": param_dict
                        }, ensure_ascii=False)
                        logger.info(f"ToolMessage migrated. result_id: {result_id}, summary: {placeholder_summary}")
                    except Exception as e:
                        logger.error(f"Error during ToolMessage migration: {e}")
    # --- END: ToolMessage ë§ˆì´ê·¸ë ˆì´ì…˜ ---

    # ğŸš¨ START: ì´ì „ í„´ì—ì„œ ë°œìƒí•œ ì—ëŸ¬ AIMessageë¥¼ ì œê±°í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í´ë¦°í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    cleaned_messages = []
    error_patterns = [
        "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì½˜í…ì¸  í•„í„°ë§ ì •ì±…ìœ¼ë¡œ ë‹µë³€ì´ ì¼ì‹œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ë°”ê¿” ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    ]
    
    last_human_idx = -1
    for i, msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            last_human_idx = i
            
    for i, msg in enumerate(messages):
        if i <= last_human_idx:
            cleaned_messages.append(msg)
        elif isinstance(msg, AIMessage):
            is_error_message = any(pattern in msg.content for pattern in error_patterns)
            if not is_error_message:
                cleaned_messages.append(msg)
            else:
                logger.info(f"Removed previous error AIMessage from context: {msg.content}")
        else:
            cleaned_messages.append(msg)
            
    state["messages"] = cleaned_messages
    messages = state["messages"]
    # ğŸš¨ END: ì—ëŸ¬ AIMessage ì œê±° ë¡œì§

    # --- START: Try-Catch-Retry & Cache Restoration Loop ---
    intermediate_messages = [] 
    try:
        logger.info("Calling model with original message...")
        response = await model.ainvoke(messages, config)
        
        # ğŸš¨ [NEW] ë‚´ë¶€ ìºì‹œ ë³µì› ë£¨í”„: LLMì´ get_cached_tool_resultë¥¼ í˜¸ì¶œí•˜ë©´ ì¦‰ì‹œ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ê³  ëª¨ë¸ì„ ì¬í˜¸ì¶œí•©ë‹ˆë‹¤.
        if isinstance(response, AIMessage) and response.tool_calls:
            cache_calls = [tc for tc in response.tool_calls if tc['name'] == 'get_cached_tool_result']
            if cache_calls:
                logger.info(f"Detected {len(cache_calls)} cache restoration calls. Handling internally and re-invoking LLM.")
                intermediate_messages.append(response)
                
                loop_messages = list(messages)
                loop_messages.append(response)
                
                session_id = config["configurable"]["thread_id"]
                async with aiosqlite.connect(settings.sqlite_directory, check_same_thread=False) as conn:
                    for tc in cache_calls:
                        result_id = tc['args'].get('result_id')
                        if result_id:
                            async with conn.cursor() as cursor:
                                await cursor.execute(
                                    "SELECT content FROM tool_results_cache WHERE result_id = ? AND session_id = ?",
                                    (result_id, session_id)
                                )
                                row = await cursor.fetchone()
                                if row:
                                    logger.info(f"Successfully restored cache for result_id: {result_id}")
                                    tool_msg = ToolMessage(content=row[0], tool_call_id=tc['id'])
                                    loop_messages.append(tool_msg)
                                    intermediate_messages.append(tool_msg)
                                else:
                                    logger.warning(f"Cache not found for result_id: {result_id}")
                                    tool_msg = ToolMessage(content=json.dumps({"error": "Cache not found"}), tool_call_id=tc['id'])
                                    loop_messages.append(tool_msg)
                                    intermediate_messages.append(tool_msg)
                
                logger.info("Re-invoking model with restored cache context...")
                response = await model.ainvoke(loop_messages, config)
        # ğŸš¨ [END] ë‚´ë¶€ ìºì‹œ ë³µì› ë£¨í”„

    except Exception as e:
        logger.warning(f"LLM call failed with error: {e}. Checking for content filter.")
        error_message = str(e)
        
        if "An assistant message with 'tool_calls' must be followed by tool messages" in error_message or 
           "tool_call_ids did not have response messages" in error_message or 
           ("invalid_request_error" in error_message and "tool_calls" in error_message):
            
            logger.error(f"Detected invalid_request_error related to tool_calls.")
            last_ai_message_idx = -1
            for i in range(len(state["messages"]) - 1, -1, -1):
                if isinstance(state["messages"][i], AIMessage):
                    last_ai_message_idx = i
                    break
            
            if last_ai_message_idx != -1:
                state["messages"][last_ai_message_idx].tool_calls = []
                state["messages"][last_ai_message_idx].content = "ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            
            return {
                "messages": state["messages"],
                "retry": state.get("retry", 0),
                "location_history": state['location_history'],
                "entity_history": state['entity_history']
            }
        
        if "content management policy" in error_message or "content filter" in error_message:
            logger.warning("Original message filtered. Retrying with sanitized message.")
            original_user_message = next((msg.content for msg in reversed(state["messages"]) if isinstance(msg, HumanMessage)), "")
            sanitized_content = sanitize_prompt(original_user_message)
            
            sanitized_messages_for_llm = list(state["messages"])
            for i in range(len(sanitized_messages_for_llm) - 1, -1, -1):
                if isinstance(sanitized_messages_for_llm[i], HumanMessage):
                    sanitized_messages_for_llm[i] = HumanMessage(content=sanitized_content, id=sanitized_messages_for_llm[i].id)
                    break
            
            try:
                response = await model.ainvoke(sanitized_messages_for_llm, config)
            except Exception as retry_e:
                logger.error(f"Retry failed: {retry_e}")
                fallback_message = "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì½˜í…ì¸  í•„í„°ë§ ì •ì±…ìœ¼ë¡œ ë‹µë³€ì´ ì¼ì‹œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ë°”ê¿” ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                response = AIMessage(content=fallback_message)
        else:
            logger.error(f"Non-filter error: {e}")
            response = AIMessage(content="ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.")
    # --- END: Try-Catch-Retry & Cache Restoration Loop ---

    locale = state.get("locale") or "ko"
    greeting = LANGUAGE_GREETINGS.get(locale, DEFAULT_GREETING)
    if is_first_interaction_in_session and not response.content.strip().startswith(greeting):
        response.content = f"{greeting}

 {response.content}"

    state["last_ai_message"] = response.content 

    return {
        "messages": intermediate_messages + [response], 
        "retry": state.get("retry", 0),
        "location_history": state['location_history'],
        "entity_history": state['entity_history'],
        "last_ai_message": state["last_ai_message"]
    }
